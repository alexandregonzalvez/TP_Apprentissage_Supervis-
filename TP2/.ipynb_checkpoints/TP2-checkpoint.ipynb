{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5b2e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import zero_one_loss\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "619c4f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitSetRandom(data,labels, train_size):\n",
    "    random_indexes = np.random.randint(labels.size, size=labels.size)\n",
    "    \n",
    "    X_train = np.array([data[i] for i in random_indexes[:train_size]])\n",
    "    X_test = np.array([data[i] for i in random_indexes[train_size:]])\n",
    "    y_train = np.array([labels[i] for i in random_indexes[:train_size]])\n",
    "    y_test =  np.array([labels[i] for i in random_indexes[train_size:]])\n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f9c1e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting mnist\n",
    "mnist = fetch_openml('mnist_784', as_frame=False) # Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85c5d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into training and testing\n",
    "X_train,X_test,y_train,y_test = splitSetRandom(data = mnist.data, labels = mnist.target, train_size = 49000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40499506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.88326656\n",
      "Iteration 2, loss = 0.87113835\n",
      "Iteration 3, loss = 0.61200780\n",
      "Iteration 4, loss = 0.46536343\n",
      "Iteration 5, loss = 0.38453579\n",
      "Iteration 6, loss = 0.32283738\n",
      "Iteration 7, loss = 0.28717335\n",
      "Iteration 8, loss = 0.26376349\n",
      "Iteration 9, loss = 0.24455726\n",
      "score = 0.9218571428571428 in 7.595390319824219 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexandre\\anaconda3\\envs\\Apprentissage5A\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "start_t = time.time()\n",
    "clf = MLPClassifier(hidden_layer_sizes = 50,verbose=True)\n",
    "clf.fit(X_train,y_train)\n",
    "print(f'score = {clf.score(X_test,y_test)} in {time.time()-start_t} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d33a5e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class = 1\n"
     ]
    }
   ],
   "source": [
    "# Predict image 4\n",
    "image_4 = X_test[3]\n",
    "plt.imshow(image_4.reshape(28, 28) ,cmap=plt.cm.gray_r,interpolation=\"nearest\") \n",
    "plt.show()\n",
    "to_predict = image_4.reshape(1, -1)\n",
    "print(f'class = {clf.predict(to_predict)[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49f33945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9218571428571428"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "precision_score(y_test,y_pred,average ='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66166879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.80904436\n",
      "Iteration 2, loss = 2.41477611\n",
      "Iteration 3, loss = 2.34045876\n",
      "Iteration 4, loss = 2.31397068\n",
      "Iteration 5, loss = 2.30474979\n",
      "Iteration 6, loss = 2.30197970\n",
      "Iteration 7, loss = 2.30127056\n",
      "Iteration 8, loss = 2.30109811\n",
      "Iteration 9, loss = 2.30105230\n",
      "Iteration 10, loss = 2.30104488\n",
      "Iteration 11, loss = 2.30102280\n",
      "Iteration 12, loss = 2.30102083\n",
      "Iteration 13, loss = 2.30101252\n",
      "Training loss did not improve more than tol=0.000100 for 4 consecutive epochs. Stopping.\n",
      "For 2 n_hidden_layers the precision is 0.11057142857142857 in 5.0380167961120605 seconds\n",
      "Iteration 1, loss = 2.92750713\n",
      "Iteration 2, loss = 2.37505365\n",
      "Iteration 3, loss = 2.31364563\n",
      "Iteration 4, loss = 2.30334589\n",
      "Iteration 5, loss = 2.30144638\n",
      "Iteration 6, loss = 2.30115805\n",
      "Iteration 7, loss = 2.30109134\n",
      "Iteration 8, loss = 2.30111991\n",
      "Iteration 9, loss = 2.30110593\n",
      "Iteration 10, loss = 2.30109247\n",
      "Iteration 11, loss = 2.30113308\n",
      "Training loss did not improve more than tol=0.000100 for 4 consecutive epochs. Stopping.\n",
      "For 7 n_hidden_layers the precision is 0.11057142857142857 in 6.158670902252197 seconds\n",
      "Iteration 1, loss = 2.33109404\n",
      "Iteration 2, loss = 2.30936234\n",
      "Iteration 3, loss = 2.30295727\n",
      "Iteration 4, loss = 2.30141314\n",
      "Iteration 5, loss = 2.30108330\n",
      "Iteration 6, loss = 2.30102962\n",
      "Iteration 7, loss = 2.30102106\n",
      "Iteration 8, loss = 2.30102731\n",
      "Iteration 9, loss = 2.30102005\n",
      "Iteration 10, loss = 2.30102324\n",
      "Training loss did not improve more than tol=0.000100 for 4 consecutive epochs. Stopping.\n",
      "For 12 n_hidden_layers the precision is 0.11057142857142857 in 8.231232166290283 seconds\n",
      "Iteration 1, loss = 2.37238139\n",
      "Iteration 2, loss = 2.33076146\n",
      "Iteration 3, loss = 2.31234002\n",
      "Iteration 4, loss = 2.30477682\n",
      "Iteration 5, loss = 2.30209430\n",
      "Iteration 6, loss = 2.30128313\n",
      "Iteration 7, loss = 2.30106789\n",
      "Iteration 8, loss = 2.30102207\n",
      "Iteration 9, loss = 2.30102490\n",
      "Iteration 10, loss = 2.30102618\n",
      "Iteration 11, loss = 2.30102362\n",
      "Iteration 12, loss = 2.30102555\n",
      "Training loss did not improve more than tol=0.000100 for 4 consecutive epochs. Stopping.\n",
      "For 17 n_hidden_layers the precision is 0.11057142857142857 in 10.798433542251587 seconds\n",
      "Iteration 1, loss = 2.37652324\n",
      "Iteration 2, loss = 2.31732370\n",
      "Iteration 3, loss = 2.30394454\n",
      "Iteration 4, loss = 2.30143763\n",
      "Iteration 5, loss = 2.30109501\n",
      "Iteration 6, loss = 2.30108014\n",
      "Iteration 7, loss = 2.30108247\n",
      "Iteration 8, loss = 2.30108549\n",
      "Iteration 9, loss = 2.30109281\n",
      "Iteration 10, loss = 2.30110728\n",
      "Training loss did not improve more than tol=0.000100 for 4 consecutive epochs. Stopping.\n",
      "For 22 n_hidden_layers the precision is 0.11057142857142857 in 10.60892391204834 seconds\n",
      "Iteration 1, loss = 2.36547984\n",
      "Iteration 2, loss = 2.32599845\n",
      "Iteration 3, loss = 2.30941008\n",
      "Iteration 4, loss = 2.30349344\n",
      "Iteration 5, loss = 2.30162643\n",
      "Iteration 6, loss = 2.30114347\n",
      "Iteration 7, loss = 2.30103503\n",
      "Iteration 8, loss = 2.30102270\n",
      "Iteration 9, loss = 2.30101570\n",
      "Iteration 10, loss = 2.30102401\n",
      "Iteration 11, loss = 2.30101575\n",
      "Iteration 12, loss = 2.30102869\n",
      "Training loss did not improve more than tol=0.000100 for 4 consecutive epochs. Stopping.\n",
      "For 27 n_hidden_layers the precision is 0.11057142857142857 in 14.251497745513916 seconds\n",
      "Iteration 1, loss = 2.42173502\n",
      "Iteration 2, loss = 2.33222811\n",
      "Iteration 3, loss = 2.30815514\n",
      "Iteration 4, loss = 2.30231978\n",
      "Iteration 5, loss = 2.30124602\n",
      "Iteration 6, loss = 2.30111220\n",
      "Iteration 7, loss = 2.30107477\n",
      "Iteration 8, loss = 2.30110470\n",
      "Iteration 9, loss = 2.30107019\n",
      "Iteration 10, loss = 2.30109742\n",
      "Iteration 11, loss = 2.30109612\n",
      "Training loss did not improve more than tol=0.000100 for 4 consecutive epochs. Stopping.\n",
      "For 32 n_hidden_layers the precision is 0.11057142857142857 in 14.770124197006226 seconds\n",
      "Iteration 1, loss = 2.47348473\n",
      "Iteration 2, loss = 2.33636683\n",
      "Iteration 3, loss = 2.30723396\n",
      "Iteration 4, loss = 2.30190529\n",
      "Iteration 5, loss = 2.30112320\n",
      "Iteration 6, loss = 2.30108584\n",
      "Iteration 7, loss = 2.30110442\n",
      "Iteration 8, loss = 2.30111302\n",
      "Iteration 9, loss = 2.30111558\n",
      "Iteration 10, loss = 2.30108353\n",
      "Training loss did not improve more than tol=0.000100 for 4 consecutive epochs. Stopping.\n",
      "For 37 n_hidden_layers the precision is 0.11057142857142857 in 15.284436225891113 seconds\n",
      "Iteration 1, loss = 2.38392599\n",
      "Iteration 2, loss = 2.32351235\n",
      "Iteration 3, loss = 2.30665579\n",
      "Iteration 4, loss = 2.30192134\n",
      "Iteration 5, loss = 2.30115133\n",
      "Iteration 6, loss = 2.30107613\n",
      "Iteration 7, loss = 2.30110984\n",
      "Iteration 8, loss = 2.30116216\n",
      "Iteration 9, loss = 2.30111488\n",
      "Iteration 10, loss = 2.30110571\n",
      "Training loss did not improve more than tol=0.000100 for 4 consecutive epochs. Stopping.\n",
      "For 42 n_hidden_layers the precision is 0.11057142857142857 in 16.79715657234192 seconds\n",
      "Iteration 1, loss = 2.53063076\n",
      "Iteration 2, loss = 2.34655547\n",
      "Iteration 3, loss = 2.31262355\n",
      "Iteration 4, loss = 2.30413205\n",
      "Iteration 5, loss = 2.30185705\n",
      "Iteration 6, loss = 2.30125996\n",
      "Iteration 7, loss = 2.30118412\n",
      "Iteration 8, loss = 2.30110668\n",
      "Iteration 9, loss = 2.30114168\n",
      "Iteration 10, loss = 2.30111386\n",
      "Iteration 11, loss = 2.30112805\n",
      "Training loss did not improve more than tol=0.000100 for 4 consecutive epochs. Stopping.\n",
      "For 47 n_hidden_layers the precision is 0.11057142857142857 in 20.351648569107056 seconds\n",
      "Iteration 1, loss = 2.50540166\n",
      "Iteration 2, loss = 2.34626742\n",
      "Iteration 3, loss = 2.31318930\n",
      "Iteration 4, loss = 2.30389263\n",
      "Iteration 5, loss = 2.30161978\n",
      "Iteration 6, loss = 2.30116411\n",
      "Iteration 7, loss = 2.30109171\n",
      "Iteration 8, loss = 2.30107680\n",
      "Iteration 9, loss = 2.30114788\n",
      "Iteration 10, loss = 2.30112755\n",
      "Iteration 11, loss = 2.30112429\n",
      "Training loss did not improve more than tol=0.000100 for 4 consecutive epochs. Stopping.\n",
      "For 52 n_hidden_layers the precision is 0.11057142857142857 in 23.042340755462646 seconds\n",
      "Iteration 1, loss = 2.45607986\n",
      "Iteration 2, loss = 2.32500957\n",
      "Iteration 3, loss = 2.30609963\n",
      "Iteration 4, loss = 2.30217008\n",
      "Iteration 5, loss = 2.30126467\n",
      "Iteration 6, loss = 2.30109188\n",
      "Iteration 7, loss = 2.30106414\n",
      "Iteration 8, loss = 2.30106974\n",
      "Iteration 9, loss = 2.30109296\n",
      "Iteration 10, loss = 2.30105758\n",
      "Iteration 11, loss = 2.30109407\n",
      "Training loss did not improve more than tol=0.000100 for 4 consecutive epochs. Stopping.\n",
      "For 57 n_hidden_layers the precision is 0.11057142857142857 in 23.583954334259033 seconds\n",
      "Iteration 1, loss = 2.47426809\n",
      "Iteration 2, loss = 2.35230402\n",
      "Iteration 3, loss = 2.31555988\n",
      "Iteration 4, loss = 2.30462857\n",
      "Iteration 5, loss = 2.30177853\n",
      "Iteration 6, loss = 2.30113363\n",
      "Iteration 7, loss = 2.30109787\n",
      "Iteration 8, loss = 2.30104861\n",
      "Iteration 9, loss = 2.30104516\n",
      "Iteration 10, loss = 2.30102509\n",
      "Iteration 11, loss = 2.30104862\n",
      "Training loss did not improve more than tol=0.000100 for 4 consecutive epochs. Stopping.\n",
      "For 62 n_hidden_layers the precision is 0.11057142857142857 in 25.1252703666687 seconds\n",
      "Iteration 1, loss = 2.37377483\n",
      "Iteration 2, loss = 2.32934370\n",
      "Iteration 3, loss = 2.31129403\n",
      "Iteration 4, loss = 2.30448201\n",
      "Iteration 5, loss = 2.30211549\n",
      "Iteration 6, loss = 2.30134680\n",
      "Iteration 7, loss = 2.30111780\n",
      "Iteration 8, loss = 2.30104796\n",
      "Iteration 9, loss = 2.30102148\n",
      "Iteration 10, loss = 2.30102153\n",
      "Iteration 11, loss = 2.30102430\n",
      "Iteration 12, loss = 2.30102307\n",
      "Training loss did not improve more than tol=0.000100 for 4 consecutive epochs. Stopping.\n",
      "For 67 n_hidden_layers the precision is 0.11057142857142857 in 30.292224645614624 seconds\n",
      "Iteration 1, loss = 2.39863085\n",
      "Iteration 2, loss = 2.32712314\n",
      "Iteration 3, loss = 2.30907042\n",
      "Iteration 4, loss = 2.30329353\n",
      "Iteration 5, loss = 2.30163017\n",
      "Iteration 6, loss = 2.30117862\n",
      "Iteration 7, loss = 2.30107946\n",
      "Iteration 8, loss = 2.30105802\n",
      "Iteration 9, loss = 2.30108122\n",
      "Iteration 10, loss = 2.30109811\n",
      "Iteration 11, loss = 2.30111401\n",
      "Training loss did not improve more than tol=0.000100 for 4 consecutive epochs. Stopping.\n",
      "For 72 n_hidden_layers the precision is 0.11057142857142857 in 29.16680908203125 seconds\n",
      "Iteration 1, loss = 2.54377542\n",
      "Iteration 2, loss = 2.36646134\n",
      "Iteration 3, loss = 2.31827128\n",
      "Iteration 4, loss = 2.30520289\n",
      "Iteration 5, loss = 2.30195481\n",
      "Iteration 6, loss = 2.30126632\n",
      "Iteration 7, loss = 2.30109177\n",
      "Iteration 8, loss = 2.30108774\n",
      "Iteration 9, loss = 2.30109806\n",
      "Iteration 10, loss = 2.30106701\n",
      "Iteration 11, loss = 2.30112005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 2.30110015\n",
      "Training loss did not improve more than tol=0.000100 for 4 consecutive epochs. Stopping.\n",
      "For 77 n_hidden_layers the precision is 0.11057142857142857 in 34.53034996986389 seconds\n",
      "Iteration 1, loss = 2.34441927\n",
      "Iteration 2, loss = 2.31615748\n",
      "Iteration 3, loss = 2.30548190\n",
      "Iteration 4, loss = 2.30210280\n",
      "Iteration 5, loss = 2.30122359\n",
      "Iteration 6, loss = 2.30104045\n",
      "Iteration 7, loss = 2.30102525\n",
      "Iteration 8, loss = 2.30101177\n",
      "Iteration 9, loss = 2.30101827\n",
      "Iteration 10, loss = 2.30103389\n",
      "Iteration 11, loss = 2.30102799\n",
      "Training loss did not improve more than tol=0.000100 for 4 consecutive epochs. Stopping.\n",
      "For 82 n_hidden_layers the precision is 0.11057142857142857 in 33.04928970336914 seconds\n",
      "Iteration 1, loss = 2.34465555\n",
      "Iteration 2, loss = 2.30671427\n",
      "Iteration 3, loss = 2.30144500\n",
      "Iteration 4, loss = 2.30110318\n",
      "Iteration 5, loss = 2.30107593\n",
      "Iteration 6, loss = 2.30108435\n",
      "Iteration 7, loss = 2.30107585\n",
      "Iteration 8, loss = 2.30108747\n",
      "Iteration 9, loss = 2.30111882\n",
      "Training loss did not improve more than tol=0.000100 for 4 consecutive epochs. Stopping.\n",
      "For 87 n_hidden_layers the precision is 0.11057142857142857 in 28.07977843284607 seconds\n",
      "Iteration 1, loss = 2.50184578\n",
      "Iteration 2, loss = 2.33272533\n",
      "Iteration 3, loss = 2.30461128\n",
      "Iteration 4, loss = 2.30136282\n",
      "Iteration 5, loss = 2.30107401\n",
      "Iteration 6, loss = 2.30106515\n",
      "Iteration 7, loss = 2.30109175\n",
      "Iteration 8, loss = 2.30105706\n",
      "Iteration 9, loss = 2.30110361\n",
      "Iteration 10, loss = 2.30115485\n",
      "Training loss did not improve more than tol=0.000100 for 4 consecutive epochs. Stopping.\n",
      "For 92 n_hidden_layers the precision is 0.11057142857142857 in 33.04779767990112 seconds\n",
      "Iteration 1, loss = 2.34898256\n",
      "Iteration 2, loss = 2.31222210\n",
      "Iteration 3, loss = 2.30346505\n",
      "Iteration 4, loss = 2.30144184\n",
      "Iteration 5, loss = 2.30115750\n",
      "Iteration 6, loss = 2.30106236\n",
      "Iteration 7, loss = 2.30111952\n",
      "Iteration 8, loss = 2.30107499\n",
      "Iteration 9, loss = 2.30108927\n",
      "Iteration 10, loss = 2.30114353\n",
      "Training loss did not improve more than tol=0.000100 for 4 consecutive epochs. Stopping.\n",
      "For 97 n_hidden_layers the precision is 0.11057142857142857 in 34.29090619087219 seconds\n"
     ]
    }
   ],
   "source": [
    "def varying_n_hidden_layer_of_1_neuron_mlp(X_train,X_test,y_train,y_test):\n",
    "    results = []\n",
    "    for n_hidden_layers in range(2,101,5):\n",
    "        start_t = time.time()\n",
    "        clf = MLPClassifier(hidden_layer_sizes = tuple([1 for i in range(n_hidden_layers)]),n_iter_no_change=4,verbose=True)\n",
    "        clf.fit(X_train,y_train)\n",
    "        precision = precision_score(y_test,clf.predict(X_test),average ='micro')\n",
    "        duration = time.time()-start_t\n",
    "        results.append({\"n_hidden_layers\":n_hidden_layers, \"precision\":precision, \"time\":duration})\n",
    "        print(f'For {n_hidden_layers} n_hidden_layers the precision is {precision} in {duration} seconds')\n",
    "    return results\n",
    "\n",
    "results = varying_n_hidden_layer_of_1_neuron_mlp(X_train,X_test,y_train,y_test)\n",
    "pickle.dump(results, open( \"results/results_varying_hidden_layers_1_neurons.p\", \"wb\" )) # Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41e9550f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "precisions = []\n",
    "n_hidden_layers_list = []\n",
    "for result in results:\n",
    "    precisions.append(result[\"precision\"])\n",
    "    n_hidden_layers_list.append(result[\"n_hidden_layers\"])\n",
    "    \n",
    "plt.plot(n_hidden_layers_list,precisions)\n",
    "plt.xlabel('n_hidden_layers')\n",
    "plt.ylabel('precision')\n",
    "plt.title('Evolution of precision by numbers of hidden layers of 1 neuron')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2dd8eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRandomModel():\n",
    "    hidden_layers_neurons_tuple = tuple([random.randint(10,300) for i in range(random.randint(1,10))])\n",
    "    model = MLPClassifier(hidden_layer_sizes = hidden_layers_neurons_tuple,verbose=True)\n",
    "    print(hidden_layers_neurons_tuple)\n",
    "    return model,hidden_layers_neurons_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80784914",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1,desc1 = createRandomModel()\n",
    "model2,desc2 = createRandomModel()\n",
    "model3,desc3 = createRandomModel()\n",
    "model4,desc4 = createRandomModel()\n",
    "model5,desc5 = createRandomModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10248b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,X_train,X_test,y_train,y_test):\n",
    "    start_t = time.time()\n",
    "    model.fit(X_train,y_train)\n",
    "    precision = precision_score(y_test,model.predict(X_test),average ='micro')\n",
    "    duration = time.time()-start_t\n",
    "    print(f'The precision is {precision} in {duration} seconds')\n",
    "    return precision, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "157646cf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.68343998\n",
      "Iteration 2, loss = 0.22750334\n",
      "Iteration 3, loss = 0.12602956\n",
      "Iteration 4, loss = 0.07975457\n",
      "Iteration 5, loss = 0.08422644\n",
      "Iteration 6, loss = 0.06229326\n",
      "Iteration 7, loss = 0.08660294\n",
      "Iteration 8, loss = 0.07845788\n",
      "Iteration 9, loss = 0.07552091\n",
      "Iteration 10, loss = 0.08647751\n",
      "Iteration 11, loss = 0.07564536\n",
      "Iteration 12, loss = 0.06021458\n",
      "Iteration 13, loss = 0.05634975\n",
      "Iteration 14, loss = 0.05912535\n",
      "Iteration 15, loss = 0.06345061\n",
      "Iteration 16, loss = 0.06199320\n",
      "Iteration 17, loss = 0.04622619\n",
      "Iteration 18, loss = 0.05047549\n",
      "Iteration 19, loss = 0.05669039\n",
      "Iteration 20, loss = 0.04877833\n",
      "Iteration 21, loss = 0.05119337\n",
      "Iteration 22, loss = 0.03726263\n",
      "Iteration 23, loss = 0.03288223\n",
      "Iteration 24, loss = 0.03086163\n",
      "Iteration 25, loss = 0.02597340\n",
      "Iteration 26, loss = 0.03379644\n",
      "Iteration 27, loss = 0.05221422\n",
      "Iteration 28, loss = 0.03187782\n",
      "Iteration 29, loss = 0.03099015\n",
      "Iteration 30, loss = 0.01893997\n",
      "Iteration 31, loss = 0.01749121\n",
      "Iteration 32, loss = 0.02619753\n",
      "Iteration 33, loss = 0.03797831\n",
      "Iteration 34, loss = 0.02912699\n",
      "Iteration 35, loss = 0.01412071\n",
      "Iteration 36, loss = 0.01416019\n",
      "Iteration 37, loss = 0.02409279\n",
      "Iteration 38, loss = 0.02323576\n",
      "Iteration 39, loss = 0.01739146\n",
      "Iteration 40, loss = 0.01731776\n",
      "Iteration 41, loss = 0.01787127\n",
      "Iteration 42, loss = 0.02129335\n",
      "Iteration 43, loss = 0.02498241\n",
      "Iteration 44, loss = 0.01529476\n",
      "Iteration 45, loss = 0.02160513\n",
      "Iteration 46, loss = 0.01794181\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The precision is 0.9826190476190476 in 258.066682100296 seconds\n"
     ]
    }
   ],
   "source": [
    "result1 = train(model1,X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4ca0af5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.81826006\n",
      "Iteration 2, loss = 0.16929794\n",
      "Iteration 3, loss = 0.10709260\n",
      "Iteration 4, loss = 0.08276825\n",
      "Iteration 5, loss = 0.06306785\n",
      "Iteration 6, loss = 0.05957552\n",
      "Iteration 7, loss = 0.04641609\n",
      "Iteration 8, loss = 0.04105630\n",
      "Iteration 9, loss = 0.04267390\n",
      "Iteration 10, loss = 0.03919604\n",
      "Iteration 11, loss = 0.03044747\n",
      "Iteration 12, loss = 0.03179199\n",
      "Iteration 13, loss = 0.03609426\n",
      "Iteration 14, loss = 0.02551310\n",
      "Iteration 15, loss = 0.02444313\n",
      "Iteration 16, loss = 0.02407803\n",
      "Iteration 17, loss = 0.02723058\n",
      "Iteration 18, loss = 0.02496157\n",
      "Iteration 19, loss = 0.02351763\n",
      "Iteration 20, loss = 0.02368949\n",
      "Iteration 21, loss = 0.01496096\n",
      "Iteration 22, loss = 0.01801402\n",
      "Iteration 23, loss = 0.01726607\n",
      "Iteration 24, loss = 0.02383577\n",
      "Iteration 25, loss = 0.01385468\n",
      "Iteration 26, loss = 0.01722384\n",
      "Iteration 27, loss = 0.01637577\n",
      "Iteration 28, loss = 0.02014529\n",
      "Iteration 29, loss = 0.01709381\n",
      "Iteration 30, loss = 0.01730951\n",
      "Iteration 31, loss = 0.01383635\n",
      "Iteration 32, loss = 0.01851193\n",
      "Iteration 33, loss = 0.01467572\n",
      "Iteration 34, loss = 0.01958126\n",
      "Iteration 35, loss = 0.01149088\n",
      "Iteration 36, loss = 0.01562052\n",
      "Iteration 37, loss = 0.01764021\n",
      "Iteration 38, loss = 0.01934748\n",
      "Iteration 39, loss = 0.01138156\n",
      "Iteration 40, loss = 0.00878206\n",
      "Iteration 41, loss = 0.01217461\n",
      "Iteration 42, loss = 0.00696344\n",
      "Iteration 43, loss = 0.01142568\n",
      "Iteration 44, loss = 0.01601547\n",
      "Iteration 45, loss = 0.01186160\n",
      "Iteration 46, loss = 0.01335314\n",
      "Iteration 47, loss = 0.01015178\n",
      "Iteration 48, loss = 0.02173506\n",
      "Iteration 49, loss = 0.01176576\n",
      "Iteration 50, loss = 0.01261797\n",
      "Iteration 51, loss = 0.00785455\n",
      "Iteration 52, loss = 0.00699292\n",
      "Iteration 53, loss = 0.01085258\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The precision is 0.9862380952380952 in 252.72986316680908 seconds\n"
     ]
    }
   ],
   "source": [
    "result2 = train(model2,X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9020dc8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.93383643\n",
      "Iteration 2, loss = 0.25349438\n",
      "Iteration 3, loss = 0.16260188\n",
      "Iteration 4, loss = 0.11864512\n",
      "Iteration 5, loss = 0.10055231\n",
      "Iteration 6, loss = 0.08550620\n",
      "Iteration 7, loss = 0.07500965\n",
      "Iteration 8, loss = 0.06755055\n",
      "Iteration 9, loss = 0.05445040\n",
      "Iteration 10, loss = 0.05606959\n",
      "Iteration 11, loss = 0.04758670\n",
      "Iteration 12, loss = 0.04500262\n",
      "Iteration 13, loss = 0.04443205\n",
      "Iteration 14, loss = 0.04457875\n",
      "Iteration 15, loss = 0.03422705\n",
      "Iteration 16, loss = 0.03869647\n",
      "Iteration 17, loss = 0.02716355\n",
      "Iteration 18, loss = 0.02750816\n",
      "Iteration 19, loss = 0.02583145\n",
      "Iteration 20, loss = 0.04175766\n",
      "Iteration 21, loss = 0.02058147\n",
      "Iteration 22, loss = 0.02274178\n",
      "Iteration 23, loss = 0.02496648\n",
      "Iteration 24, loss = 0.02674486\n",
      "Iteration 25, loss = 0.02473701\n",
      "Iteration 26, loss = 0.02501358\n",
      "Iteration 27, loss = 0.01933495\n",
      "Iteration 28, loss = 0.02468345\n",
      "Iteration 29, loss = 0.01367587\n",
      "Iteration 30, loss = 0.01357947\n",
      "Iteration 31, loss = 0.02312739\n",
      "Iteration 32, loss = 0.01812563\n",
      "Iteration 33, loss = 0.01720319\n",
      "Iteration 34, loss = 0.01671002\n",
      "Iteration 35, loss = 0.01699428\n",
      "Iteration 36, loss = 0.01594754\n",
      "Iteration 37, loss = 0.01023953\n",
      "Iteration 38, loss = 0.01916271\n",
      "Iteration 39, loss = 0.02232671\n",
      "Iteration 40, loss = 0.01503619\n",
      "Iteration 41, loss = 0.00950381\n",
      "Iteration 42, loss = 0.01110133\n",
      "Iteration 43, loss = 0.01322668\n",
      "Iteration 44, loss = 0.01273210\n",
      "Iteration 45, loss = 0.01282850\n",
      "Iteration 46, loss = 0.01274844\n",
      "Iteration 47, loss = 0.01303333\n",
      "Iteration 48, loss = 0.01074190\n",
      "Iteration 49, loss = 0.01493342\n",
      "Iteration 50, loss = 0.01496973\n",
      "Iteration 51, loss = 0.01190266\n",
      "Iteration 52, loss = 0.00966123\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The precision is 0.9846190476190476 in 99.76634931564331 seconds\n"
     ]
    }
   ],
   "source": [
    "result3 = train(model3,X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1fa541b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.95544550\n",
      "Iteration 2, loss = 0.26812120\n",
      "Iteration 3, loss = 0.18354238\n",
      "Iteration 4, loss = 0.14635587\n",
      "Iteration 5, loss = 0.12113940\n",
      "Iteration 6, loss = 0.10118791\n",
      "Iteration 7, loss = 0.09311529\n",
      "Iteration 8, loss = 0.08219835\n",
      "Iteration 9, loss = 0.07638354\n",
      "Iteration 10, loss = 0.06761075\n",
      "Iteration 11, loss = 0.06079682\n",
      "Iteration 12, loss = 0.05459698\n",
      "Iteration 13, loss = 0.05540471\n",
      "Iteration 14, loss = 0.05106348\n",
      "Iteration 15, loss = 0.04571190\n",
      "Iteration 16, loss = 0.04929057\n",
      "Iteration 17, loss = 0.04227125\n",
      "Iteration 18, loss = 0.03938259\n",
      "Iteration 19, loss = 0.03827795\n",
      "Iteration 20, loss = 0.03968258\n",
      "Iteration 21, loss = 0.03587463\n",
      "Iteration 22, loss = 0.03843996\n",
      "Iteration 23, loss = 0.03558834\n",
      "Iteration 24, loss = 0.03357805\n",
      "Iteration 25, loss = 0.02840455\n",
      "Iteration 26, loss = 0.02256292\n",
      "Iteration 27, loss = 0.02880063\n",
      "Iteration 28, loss = 0.03199110\n",
      "Iteration 29, loss = 0.02924726\n",
      "Iteration 30, loss = 0.02847441\n",
      "Iteration 31, loss = 0.02111974\n",
      "Iteration 32, loss = 0.02534884\n",
      "Iteration 33, loss = 0.02563707\n",
      "Iteration 34, loss = 0.02607953\n",
      "Iteration 35, loss = 0.02166404\n",
      "Iteration 36, loss = 0.01472169\n",
      "Iteration 37, loss = 0.02250391\n",
      "Iteration 38, loss = 0.02325626\n",
      "Iteration 39, loss = 0.01842749\n",
      "Iteration 40, loss = 0.01683131\n",
      "Iteration 41, loss = 0.02135682\n",
      "Iteration 42, loss = 0.01838069\n",
      "Iteration 43, loss = 0.01990923\n",
      "Iteration 44, loss = 0.02157235\n",
      "Iteration 45, loss = 0.01672105\n",
      "Iteration 46, loss = 0.01339813\n",
      "Iteration 47, loss = 0.01533538\n",
      "Iteration 48, loss = 0.01965153\n",
      "Iteration 49, loss = 0.01682191\n",
      "Iteration 50, loss = 0.01517250\n",
      "Iteration 51, loss = 0.01381522\n",
      "Iteration 52, loss = 0.02212748\n",
      "Iteration 53, loss = 0.01252011\n",
      "Iteration 54, loss = 0.01146496\n",
      "Iteration 55, loss = 0.01420161\n",
      "Iteration 56, loss = 0.01289095\n",
      "Iteration 57, loss = 0.01638612\n",
      "Iteration 58, loss = 0.01966372\n",
      "Iteration 59, loss = 0.01476528\n",
      "Iteration 60, loss = 0.01012919\n",
      "Iteration 61, loss = 0.00553063\n",
      "Iteration 62, loss = 0.01420086\n",
      "Iteration 63, loss = 0.01481587\n",
      "Iteration 64, loss = 0.01795291\n",
      "Iteration 65, loss = 0.01196196\n",
      "Iteration 66, loss = 0.01098669\n",
      "Iteration 67, loss = 0.01602286\n",
      "Iteration 68, loss = 0.01069709\n",
      "Iteration 69, loss = 0.01094560\n",
      "Iteration 70, loss = 0.00906893\n",
      "Iteration 71, loss = 0.01270297\n",
      "Iteration 72, loss = 0.01170094\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The precision is 0.9820476190476191 in 106.04878497123718 seconds\n"
     ]
    }
   ],
   "source": [
    "result4 = train(model4,X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f5cf1e77",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.51868878\n",
      "Iteration 2, loss = 0.16425201\n",
      "Iteration 3, loss = 0.11318759\n",
      "Iteration 4, loss = 0.08578423\n",
      "Iteration 5, loss = 0.06961662\n",
      "Iteration 6, loss = 0.06151224\n",
      "Iteration 7, loss = 0.05321909\n",
      "Iteration 8, loss = 0.05080198\n",
      "Iteration 9, loss = 0.03878090\n",
      "Iteration 10, loss = 0.04198814\n",
      "Iteration 11, loss = 0.03693767\n",
      "Iteration 12, loss = 0.03402374\n",
      "Iteration 13, loss = 0.02863335\n",
      "Iteration 14, loss = 0.03148894\n",
      "Iteration 15, loss = 0.02882473\n",
      "Iteration 16, loss = 0.02659894\n",
      "Iteration 17, loss = 0.02890094\n",
      "Iteration 18, loss = 0.02876786\n",
      "Iteration 19, loss = 0.02335781\n",
      "Iteration 20, loss = 0.01774691\n",
      "Iteration 21, loss = 0.01894415\n",
      "Iteration 22, loss = 0.02066605\n",
      "Iteration 23, loss = 0.01981066\n",
      "Iteration 24, loss = 0.01707580\n",
      "Iteration 25, loss = 0.01694551\n",
      "Iteration 26, loss = 0.01455676\n",
      "Iteration 27, loss = 0.02400647\n",
      "Iteration 28, loss = 0.01950547\n",
      "Iteration 29, loss = 0.02126806\n",
      "Iteration 30, loss = 0.01528797\n",
      "Iteration 31, loss = 0.01337372\n",
      "Iteration 32, loss = 0.01440424\n",
      "Iteration 33, loss = 0.01636492\n",
      "Iteration 34, loss = 0.01386570\n",
      "Iteration 35, loss = 0.01377866\n",
      "Iteration 36, loss = 0.01168957\n",
      "Iteration 37, loss = 0.01387186\n",
      "Iteration 38, loss = 0.01811349\n",
      "Iteration 39, loss = 0.01656562\n",
      "Iteration 40, loss = 0.01813333\n",
      "Iteration 41, loss = 0.01127733\n",
      "Iteration 42, loss = 0.01180340\n",
      "Iteration 43, loss = 0.01276010\n",
      "Iteration 44, loss = 0.01639955\n",
      "Iteration 45, loss = 0.01110387\n",
      "Iteration 46, loss = 0.01086002\n",
      "Iteration 47, loss = 0.01535712\n",
      "Iteration 48, loss = 0.01747530\n",
      "Iteration 49, loss = 0.01299291\n",
      "Iteration 50, loss = 0.00842126\n",
      "Iteration 51, loss = 0.00413690\n",
      "Iteration 52, loss = 0.01675567\n",
      "Iteration 53, loss = 0.01170662\n",
      "Iteration 54, loss = 0.01052155\n",
      "Iteration 55, loss = 0.01032049\n",
      "Iteration 56, loss = 0.00542018\n",
      "Iteration 57, loss = 0.01440744\n",
      "Iteration 58, loss = 0.01261173\n",
      "Iteration 59, loss = 0.00999720\n",
      "Iteration 60, loss = 0.00954214\n",
      "Iteration 61, loss = 0.01079059\n",
      "Iteration 62, loss = 0.01548963\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "The precision is 0.9851904761904762 in 257.00539994239807 seconds\n"
     ]
    }
   ],
   "source": [
    "result5 = train(model5,X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a7f99747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263, 200, 277)\n",
      "[0.9826190476190476, 0.9862380952380952, 0.9846190476190476, 0.9820476190476191, 0.9851904761904762]\n",
      "['(263, 200, 277)\\n258s', '(152, 210, 140, 239, 138, 175, 197, 30)\\n252s', '(91, 92, 23, 182, 87)\\n99s', '(39, 102, 36, 180, 55)\\n106s', '(71, 209, 107, 18, 227, 297, 239)\\n257s']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math \n",
    "results = [result1,result2,result3,result4,result5]\n",
    "descs = [desc1,desc2,desc3,desc4,desc5]\n",
    "\n",
    "precisions = [result[0] for result in results]\n",
    "labels =  [\"{}\\n{}s\".format(descs[i],math.trunc(results[i][1]))  for i in range(5)]\n",
    "\n",
    "f, ax = plt.subplots(figsize=(18,5)) # set the size that you'd like (width, height)\n",
    "ax.legend(fontsize = 14)\n",
    "plt.bar(range(5), precisions, tick_label=labels)\n",
    "plt.title(\"precision depending on the model\")\n",
    "plt.ylabel(\"precision\")\n",
    "plt.xlabel(\"models\")\n",
    "plt.ylim([0.98,0.99])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9ccef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexandre\\anaconda3\\envs\\Apprentissage5A\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Alexandre\\anaconda3\\envs\\Apprentissage5A\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Alexandre\\anaconda3\\envs\\Apprentissage5A\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Alexandre\\anaconda3\\envs\\Apprentissage5A\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\Alexandre\\anaconda3\\envs\\Apprentissage5A\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish for solver lbfgs\n",
      "Iteration 1, loss = 2.31802527\n",
      "Iteration 2, loss = 1.94040868\n",
      "Iteration 3, loss = 1.81896220\n",
      "Iteration 4, loss = 1.83437738\n",
      "Iteration 5, loss = 1.63356341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexandre\\anaconda3\\envs\\Apprentissage5A\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.23387393\n",
      "Iteration 2, loss = 1.98203875\n",
      "Iteration 3, loss = 1.71338561\n",
      "Iteration 4, loss = 1.46172707\n",
      "Iteration 5, loss = 0.83585380\n",
      "Iteration 6, loss = 0.51465177\n",
      "Iteration 7, loss = 0.38220963\n",
      "Iteration 8, loss = 0.31196874\n",
      "Iteration 9, loss = 0.26138391\n",
      "Iteration 10, loss = 0.23270778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexandre\\anaconda3\\envs\\Apprentissage5A\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.28220354\n",
      "Iteration 2, loss = 0.37463166\n",
      "Iteration 3, loss = 0.26582291\n",
      "Iteration 4, loss = 0.21171776\n",
      "Iteration 5, loss = 0.17666970\n",
      "Iteration 6, loss = 0.15623679\n",
      "Iteration 7, loss = 0.13620791\n",
      "Iteration 8, loss = 0.12070725\n",
      "Iteration 9, loss = 0.10830832\n",
      "Iteration 10, loss = 0.09732522\n",
      "Iteration 11, loss = 0.08855460\n",
      "Iteration 12, loss = 0.08086363\n",
      "Iteration 13, loss = 0.07326982\n",
      "Iteration 14, loss = 0.06806518\n",
      "Iteration 15, loss = 0.06126431\n",
      "Iteration 16, loss = 0.05663128\n",
      "Iteration 17, loss = 0.05307335\n",
      "Iteration 18, loss = 0.04835754\n",
      "Iteration 19, loss = 0.04556384\n",
      "Iteration 20, loss = 0.04031717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexandre\\anaconda3\\envs\\Apprentissage5A\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.36903773\n",
      "Iteration 2, loss = 0.45906710\n",
      "Iteration 3, loss = 0.31410715\n",
      "Iteration 4, loss = 0.24902925\n",
      "Iteration 5, loss = 0.20985455\n",
      "Iteration 6, loss = 0.18362699\n",
      "Iteration 7, loss = 0.16333453\n",
      "Iteration 8, loss = 0.14442276\n",
      "Iteration 9, loss = 0.13106357\n",
      "Iteration 10, loss = 0.11859553\n",
      "Iteration 11, loss = 0.10648149\n",
      "Iteration 12, loss = 0.09776141\n",
      "Iteration 13, loss = 0.08897871\n",
      "Iteration 14, loss = 0.08283026\n",
      "Iteration 15, loss = 0.07618601\n",
      "Iteration 16, loss = 0.06940708\n",
      "Iteration 17, loss = 0.06525912\n",
      "Iteration 18, loss = 0.05951104\n",
      "Iteration 19, loss = 0.05467768\n",
      "Iteration 20, loss = 0.05162196\n",
      "Iteration 21, loss = 0.04792050\n",
      "Iteration 22, loss = 0.04512574\n",
      "Iteration 23, loss = 0.04041268\n",
      "Iteration 24, loss = 0.03771927\n",
      "Iteration 25, loss = 0.03485827\n",
      "Iteration 26, loss = 0.03136228\n",
      "Iteration 27, loss = 0.03051165\n",
      "Iteration 28, loss = 0.02822458\n",
      "Iteration 29, loss = 0.02501690\n",
      "Iteration 30, loss = 0.02457802\n",
      "Iteration 31, loss = 0.02138367\n",
      "Iteration 32, loss = 0.01995761\n",
      "Iteration 33, loss = 0.01978558\n",
      "Iteration 34, loss = 0.01769675\n",
      "Iteration 35, loss = 0.01621128\n",
      "Iteration 36, loss = 0.01649732\n",
      "Iteration 37, loss = 0.01376500\n",
      "Iteration 38, loss = 0.01435102\n",
      "Iteration 39, loss = 0.01184033\n",
      "Iteration 40, loss = 0.01225590\n",
      "Iteration 41, loss = 0.01138147\n",
      "Iteration 42, loss = 0.01129318\n",
      "Iteration 43, loss = 0.00987600\n",
      "Iteration 44, loss = 0.00918782\n",
      "Iteration 45, loss = 0.00833905\n",
      "Iteration 46, loss = 0.00825763\n",
      "Iteration 47, loss = 0.00801052\n",
      "Iteration 48, loss = 0.00746768\n",
      "Iteration 49, loss = 0.00655114\n",
      "Iteration 50, loss = 0.00671066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexandre\\anaconda3\\envs\\Apprentissage5A\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.60481593\n",
      "Iteration 2, loss = 0.59766419\n",
      "Iteration 3, loss = 0.33474822\n",
      "Iteration 4, loss = 0.26577335\n",
      "Iteration 5, loss = 0.22539880\n",
      "Iteration 6, loss = 0.19688720\n",
      "Iteration 7, loss = 0.17614161\n",
      "Iteration 8, loss = 0.16028953\n",
      "Iteration 9, loss = 0.14541435\n",
      "Iteration 10, loss = 0.13410048\n",
      "Iteration 11, loss = 0.12133242\n",
      "Iteration 12, loss = 0.11214001\n",
      "Iteration 13, loss = 0.10163501\n",
      "Iteration 14, loss = 0.09692547\n",
      "Iteration 15, loss = 0.08750229\n",
      "Iteration 16, loss = 0.08168877\n",
      "Iteration 17, loss = 0.07588211\n",
      "Iteration 18, loss = 0.06927818\n",
      "Iteration 19, loss = 0.06392552\n",
      "Iteration 20, loss = 0.06027176\n",
      "Iteration 21, loss = 0.05670606\n",
      "Iteration 22, loss = 0.05028776\n",
      "Iteration 23, loss = 0.04714131\n",
      "Iteration 24, loss = 0.04260988\n",
      "Iteration 25, loss = 0.03945208\n",
      "Iteration 26, loss = 0.03693866\n",
      "Iteration 27, loss = 0.03489871\n",
      "Iteration 28, loss = 0.03289729\n",
      "Iteration 29, loss = 0.03079753\n",
      "Iteration 30, loss = 0.02761938\n",
      "Iteration 31, loss = 0.02581426\n",
      "Iteration 32, loss = 0.02419586\n",
      "Iteration 33, loss = 0.02264005\n",
      "Iteration 34, loss = 0.02216583\n",
      "Iteration 35, loss = 0.02209651\n",
      "Iteration 36, loss = 0.02072567\n",
      "Iteration 37, loss = 0.01922612\n",
      "Iteration 38, loss = 0.01819189\n",
      "Iteration 39, loss = 0.01699757\n",
      "Iteration 40, loss = 0.01648385\n",
      "Iteration 41, loss = 0.01361014\n",
      "Iteration 42, loss = 0.01306476\n",
      "Iteration 43, loss = 0.01154348\n",
      "Iteration 44, loss = 0.01068264\n",
      "Iteration 45, loss = 0.01029746\n",
      "Iteration 46, loss = 0.00936541\n",
      "Iteration 47, loss = 0.01034991\n",
      "Iteration 48, loss = 0.01013657\n",
      "Iteration 49, loss = 0.00984560\n",
      "Iteration 50, loss = 0.00891986\n",
      "Iteration 51, loss = 0.00909075\n",
      "Iteration 52, loss = 0.01131559\n",
      "Iteration 53, loss = 0.00905813\n",
      "Iteration 54, loss = 0.00769583\n",
      "Iteration 55, loss = 0.00707556\n",
      "Iteration 56, loss = 0.00631809\n"
     ]
    }
   ],
   "source": [
    "def studyConvergenceDependingSolvers(X_train,X_test,y_train,y_test,hidden_layers_neurons_tuple= (91, 92, 23, 182, 87),max_iter_list = [5,10,20,50,100],solver_list = ['lbfgs','sgd','adam']):\n",
    "    for solver in solver_list:\n",
    "        results = []\n",
    "        epoch = 0\n",
    "        for max_iter in max_iter_list:\n",
    "            model = MLPClassifier(hidden_layer_sizes = hidden_layers_neurons_tuple,max_iter=max_iter,solver=solver,verbose=True)   \n",
    "            start_t = time.time()\n",
    "            model.fit(X_train,y_train)\n",
    "            training_time = time.time()-start_t\n",
    "            prediction = model.predict(X_test)\n",
    "            precision = precision_score(y_test,prediction,average ='micro')\n",
    "            recall = recall_score(y_test,prediction,average = 'micro')\n",
    "            error = zero_one_loss(y_test,prediction)\n",
    "            results.append({\"max_iter\":max_iter,\"precision\":precision,\"error\":error,\"recall\":recall,\"training_time\":training_time})\n",
    "        pickle.dump(results, open( \"results/by_solver/{}_results.p\".format(solver), \"wb\" )) # Save results\n",
    "        print(\"finish for solver {}\".format(solver))\n",
    "    \n",
    "studyConvergenceDependingSolvers(X_train,X_test,y_train,y_test,hidden_layers_neurons_tuple= (91, 92, 23, 182, 87),max_iter_list = [5,10,20,50,100],solver_list = ['lbfgs','sgd','adam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac058cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        data = pickle.load(fo, encoding='latin1')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c672a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "solvers = ['lbfgs','sgd','adam']\n",
    "\n",
    "for solver in solvers:\n",
    "    results = unpickle(f'results/by_solver/{solver}_results.p')\n",
    "    precisions = [result[\"precision\"] for result in results]\n",
    "    max_iter = [result[\"max_iter\"] for result in results]\n",
    "    plt.plot(max_iter, precisions, label=solver)\n",
    "\n",
    "plt.xlabel('total_iteration')\n",
    "plt.ylabel('precision')\n",
    "plt.title('Evolution of precision by iterations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb3bda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "solvers = ['lbfgs','sgd','adam']\n",
    "\n",
    "for solver in solvers:\n",
    "    results = unpickle(f'results/by_solver/{solver}_results.p')\n",
    "    errors = [result[\"error\"] for result in results]\n",
    "    max_iter = [result[\"max_iter\"] for result in results]\n",
    "    plt.plot(max_iter, errors, label=solver)\n",
    "\n",
    "plt.xlabel('total_iteration')\n",
    "plt.ylabel('error')\n",
    "plt.title('Evolution of error by iterations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5a2196",
   "metadata": {},
   "outputs": [],
   "source": [
    "solvers = ['lbfgs','sgd','adam']\n",
    "\n",
    "for solver in solvers:\n",
    "    results = unpickle(f'results/by_solver/{solver}_results.p')\n",
    "    recalls = [result[\"recall\"] for result in results]\n",
    "    max_iter = [result[\"max_iter\"] for result in results]\n",
    "    plt.plot(max_iter, recalls, label=solver)\n",
    "\n",
    "plt.xlabel('total_iteration')\n",
    "plt.ylabel('recall')\n",
    "plt.title('Evolution of recall by iterations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4684cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "solvers = ['lbfgs','sgd','adam']\n",
    "\n",
    "for solver in solvers:\n",
    "    results = unpickle(f'results/by_solver/{solver}_results.p')\n",
    "    precisions = [result[\"precision\"] for result in results]\n",
    "    training_time = [result[\"training_time\"] for result in results]\n",
    "    plt.plot(training_time, precisions, label=solver)\n",
    "\n",
    "plt.xlabel('training_time')\n",
    "plt.ylabel('precision')\n",
    "plt.title('Evolution of precision by training_time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c567e3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def studyConvergenceDependingActivationFunction(X_train,X_test,y_train,y_test,hidden_layers_neurons_tuple= (91, 92, 23, 182, 87),max_iter_list = [5,10,20,50,100],activation_list = ['identity','logistic','tanh','relu']):\n",
    "    for activation in activation_list:\n",
    "        results = []\n",
    "        epoch = 0\n",
    "        for max_iter in max_iter_list:\n",
    "            model = MLPClassifier(hidden_layer_sizes = hidden_layers_neurons_tuple,max_iter=max_iter,activation=activation,verbose=True)   \n",
    "            start_t = time.time()\n",
    "            model.fit(X_train,y_train)\n",
    "            training_time = time.time()-start_t\n",
    "            prediction = model.predict(X_test)\n",
    "            precision = precision_score(y_test,prediction,average ='micro')\n",
    "            recall = recall_score(y_test,prediction,average = 'micro')\n",
    "            error = zero_one_loss(y_test,prediction)\n",
    "            results.append({\"max_iter\":max_iter,\"precision\":precision,\"error\":error,\"recall\":recall,\"training_time\":training_time})\n",
    "        pickle.dump(results, open( \"results/by_activation/{}_results.p\".format(activation), \"wb\" )) # Save results\n",
    "        print(\"finish for activation {}\".format(activation))\n",
    "    \n",
    "studyConvergenceDependingActivationFunction(X_train,X_test,y_train,y_test,hidden_layers_neurons_tuple= (91, 92, 23, 182, 87),max_iter_list = [5,10,20,50,100],activation_list = ['identity','logistic','tanh','relu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a494b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = ['identity','logistic','tanh','relu']\n",
    "\n",
    "for activation in activations:\n",
    "    results = unpickle(f'results/by_activation/{activation}_results.p')\n",
    "    precisions = [result[\"precision\"] for result in results]\n",
    "    max_iter = [result[\"max_iter\"] for result in results]\n",
    "    plt.plot(max_iter, precisions, label=activation)\n",
    "\n",
    "plt.xlabel('total_iteration')\n",
    "plt.ylabel('precision')\n",
    "plt.title('Evolution of precision by iterations')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for activation in activations:\n",
    "    results = unpickle(f'results/by_activation/{activation}_results.p')\n",
    "    errors = [result[\"error\"] for result in results]\n",
    "    max_iter = [result[\"max_iter\"] for result in results]\n",
    "    plt.plot(max_iter, errors, label=activation)\n",
    "\n",
    "plt.xlabel('total_iteration')\n",
    "plt.ylabel('error')\n",
    "plt.title('Evolution of error by iterations')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for activation in activations:\n",
    "    results = unpickle(f'results/by_activation/{activation}_results.p')\n",
    "    recalls = [result[\"recall\"] for result in results]\n",
    "    max_iter = [result[\"max_iter\"] for result in results]\n",
    "    plt.plot(max_iter, recalls, label=activation)\n",
    "\n",
    "plt.xlabel('total_iteration')\n",
    "plt.ylabel('recall')\n",
    "plt.title('Evolution of recall by iterations')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for activation in activations:\n",
    "    results = unpickle(f'results/by_activation/{activation}_results.p')\n",
    "    precisions = [result[\"precision\"] for result in results]\n",
    "    training_time = [result[\"training_time\"] for result in results]\n",
    "    plt.plot(training_time, precisions, label=activation)\n",
    "\n",
    "plt.xlabel('training_time')\n",
    "plt.ylabel('precision')\n",
    "plt.title('Evolution of precision by training_time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713929ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def studyConvergenceDependingAlpha(X_train,X_test,y_train,y_test,hidden_layers_neurons_tuple= (91, 92, 23, 182, 87),max_iter_list = [5,10,20,50,100],alpha_list = [0.00001,0.0001,0.0005,0.001,0.01]):\n",
    "    for alpha in alpha_list:\n",
    "        results = []\n",
    "        epoch = 0\n",
    "        for max_iter in max_iter_list:\n",
    "            model = MLPClassifier(hidden_layer_sizes = hidden_layers_neurons_tuple,max_iter=max_iter,alpha=alpha,verbose=True)   \n",
    "            start_t = time.time()\n",
    "            model.fit(X_train,y_train)\n",
    "            training_time = time.time()-start_t\n",
    "            prediction = model.predict(X_test)\n",
    "            precision = precision_score(y_test,prediction,average ='micro')\n",
    "            recall = recall_score(y_test,prediction,average = 'micro')\n",
    "            error = zero_one_loss(y_test,prediction)\n",
    "            results.append({\"max_iter\":max_iter,\"precision\":precision,\"error\":error,\"recall\":recall,\"training_time\":training_time})\n",
    "        pickle.dump(results, open( \"results/by_alpha/{}_results.p\".format(alpha), \"wb\" )) # Save results\n",
    "        print(\"finish for alpha {}\".format(alpha))\n",
    "    \n",
    "studyConvergenceDependingAlpha(X_train,X_test,y_train,y_test,hidden_layers_neurons_tuple= (91, 92, 23, 182, 87),max_iter_list = [5,10,20,50,100],alpha_list = [0.00001,0.0001,0.0005,0.001,0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43d9b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.00001,0.0001,0.0005,0.001,0.01]\n",
    "\n",
    "for alpha in alphas:\n",
    "    results = unpickle(f'results/by_alpha/{alpha}_results.p')\n",
    "    precisions = [result[\"precision\"] for result in results]\n",
    "    max_iter = [result[\"max_iter\"] for result in results]\n",
    "    plt.plot(max_iter, precisions, label=alpha)\n",
    "\n",
    "plt.xlabel('total_iteration')\n",
    "plt.ylabel('precision')\n",
    "plt.title('Evolution of precision by iterations')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for alpha in alphas:\n",
    "    results = unpickle(f'results/by_alpha/{alpha}_results.p')\n",
    "    errors = [result[\"error\"] for result in results]\n",
    "    max_iter = [result[\"max_iter\"] for result in results]\n",
    "    plt.plot(max_iter, errors, label=alpha)\n",
    "\n",
    "plt.xlabel('total_iteration')\n",
    "plt.ylabel('error')\n",
    "plt.title('Evolution of error by iterations')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for alpha in alphas:\n",
    "    results = unpickle(f'results/by_alpha/{alpha}_results.p')\n",
    "    recalls = [result[\"recall\"] for result in results]\n",
    "    max_iter = [result[\"max_iter\"] for result in results]\n",
    "    plt.plot(max_iter, recalls, label=alpha)\n",
    "\n",
    "plt.xlabel('total_iteration')\n",
    "plt.ylabel('recall')\n",
    "plt.title('Evolution of recall by iterations')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for alpha in alphas:\n",
    "    results = unpickle(f'results/by_alpha/{alpha}_results.p')\n",
    "    precisions = [result[\"precision\"] for result in results]\n",
    "    training_time = [result[\"training_time\"] for result in results]\n",
    "    plt.plot(training_time, precisions, label=alpha)\n",
    "\n",
    "plt.xlabel('training_time')\n",
    "plt.ylabel('precision')\n",
    "plt.title('Evolution of precision by training_time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
